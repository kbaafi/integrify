{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Stochastic gradient descent(SGD) and General / Batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an input dataset $\\mathbf{x}$, a target $\\mathbf{y}$, hypothesis function $\\mathbf{h_{\\theta}(x)}$ and a cost function $\\mathbf{J_{\\theta}(x,y,m)}$, $\\mathbf{J_{\\theta}}$ being a function of the hypothesis function and the target and $m$ being the number of data examples,\n",
    "\n",
    "the optimization step in gradient descent function is as follows:\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac {\\partial}{\\partial \\theta_j}J_{\\theta}(x,y,m)$$\n",
    "\n",
    "This function is optimized until $\\mathbf{J_{\\theta}}$ is less than a tolerance value or the set number of iterations have been reached. In all cases, the optimization is performed on all examples in the data set, and that is why regular gradient descent is also known as batch gradient descent. The effect of this is an incremental movement towards the global minimum. \n",
    "\n",
    "With stochastic gradient descent however, the value for $m$ is set to 1, and in each iteration, for each element in the dataset the weights are optimized. \n",
    "\n",
    "The optimization step in stochastic gradient descent looks as follows:\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac {\\partial}{\\partial \\theta_j}J_{\\theta}(x_i,y_i)$$\n",
    "where $i$ is the current element in the data set\n",
    "\n",
    "The optimization process looks as follows:\n",
    "```python\n",
    "    for k in num_iterations:\n",
    "        for i in num_elements:\n",
    "            optimize weights on element i\n",
    "```\n",
    "\n",
    "This allows the algorithm to move towards the optimum at the calculation of the cost related to each example instead of the cost related to **all** the examples if we had used regular gradient descent. We can write the SGD process as follows:\n",
    "\n",
    "```python\n",
    "def SGD(f, theta0, alpha, num_iters):\n",
    "    \"\"\" \n",
    "       Arguments:\n",
    "       f -- the function to optimize, it takes a single argument\n",
    "            and yield two outputs, a cost and the gradient\n",
    "            with respect to the arguments\n",
    "       theta0 -- the initial point to start SGD from\n",
    "       num_iters -- total iterations to run SGD for       \n",
    "       Returns:\n",
    "       theta -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "    start_iter = 0\n",
    "    theta= theta0    \n",
    "    for iter in xrange(start_iter + 1, num_iters + 1):\n",
    "        _, grad = f(theta)\n",
    "        theta = theta - (alpha * grad) # there is NO dot product!\n",
    "    return theta\n",
    "```\n",
    "\n",
    "**So why the name stochastic gradient descent?**\n",
    "\n",
    "The name is derived from the fact that at each optimization step, a train, target  set is chosen at random and the weights are optimized accordingly. More specifically the dataset is first shuffled before running SGD to make sure that the data is randomized.\n",
    "\n",
    "**On Convergence**\n",
    "\n",
    "Batch GD takes a reasonably straightforward trajectory to the optimium, but in SGD, because each iteration is trying to create a better fit to the cost function, it tends to take seemingly random/ circuitous trajectory towards the optimum. Actually, it never reaches the optimum but remains in the general neighborhood of the optimum until the algorithm is stopped. **SGD never converges** but it ends up falling within a location close enough that the approximation of the optimum achieved using SGD can represent the global optimum\n",
    "\n",
    "![sgd](/images/sgd.png)\n",
    "\n",
    "An example trajectory for SGD is shown in pink, with a corresponding trajectory for batch gradient shown in red\n",
    "\n",
    "**Speed of 'Convergence'**\n",
    "\n",
    "With batch GD, one optimization step is taken after evaluating the cost function over all examples of the dataset. With stochastic GD, the weights are updated for each training example, and this can lead to much faster movement towards the optimum. Usually after a few iterations or **epochs** over the entire dataset, the optimum will have been found\n",
    "\n",
    "**When to use SGD**\n",
    "\n",
    "If the number of training samples are large, in fact very large, then using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample.\n",
    "\n",
    "SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of Batch GD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

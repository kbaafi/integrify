{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessing predefined path\n"
     ]
    }
   ],
   "source": [
    "log_path = './runs/gd/'\n",
    "\n",
    "if log_path:\n",
    "    print(\"accessing predefined path\")\n",
    "    writer = SummaryWriter(log_dir=log_path)\n",
    "else :\n",
    "    print(\"using new path set\")\n",
    "    writer = SummaryWriter(log_dir='./runs/gd/')\n",
    "    \n",
    "#In addition to SummaryWriter, there are also other writers, please check the manual\n",
    "# https://tensorboardx.readthedocs.io/en/latest/tutorial.html\n",
    "\n",
    "# !tensorboard --logdir log_path --host localhost --port 8088\n",
    "# you have to execute tensorboard command from another shell, otherwise you cannot proceed with running the notebook\n",
    "# read this : https://medium.com/@anthony_sarkis/tensorboard-quick-start-in-5-minutes-e3ec69f673af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "device = torch.device('cpu') # Uncomment this to run on CPU\n",
    "\n",
    "# N is batch size;\n",
    "# D_in is input dimension;\n",
    "# H is hidden dimension; \n",
    "#D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 350, 50, 20\n",
    "\n",
    "# input data : batch of 64 times 1000 features\n",
    "# output data : 100 x 10 continues values (real scalars)\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.rand(N, D_in, requires_grad = False) # generate normally distributed data of dim NxD_in, store it on device, requires_grad = False\n",
    "y = torch.rand(N, D_out, requires_grad = False) # generate normally distributed data of dim NxD_out, store it on device, requires_grad = False\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "# here the gradient will be computed for the variables that are related to the model learning something new,\n",
    "# i.e. the network weights in this case\n",
    "\n",
    "\n",
    "# WEIGHTS\n",
    "w1 = torch.rand(D_in,H, requires_grad = True)#generate normally distributed data of dim D_in x H, store it on device, requires_grad = True \n",
    "w2 = torch.rand(H, Dout requires_grad = True)#generate normally distributed data of dim H x D_out, store it on device, requires_grad = True \n",
    "\n",
    "# initialize loss value to a high number \n",
    "\n",
    "# initialize arrays errors, w1_array and w2_array to empty lists\n",
    "errors = torch.rand(N, D_out, requires_grad = False)# write here\n",
    "w1_array =  # write here\n",
    "w2_array =  # write here\n",
    " # set the network learning rate parameter 'learning_rate' to some small number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    # predict the values by multiplying x with weight matrix w1, then apply RELU activation and multiply the result by weight matrix w2\n",
    "    y_pred = # your code here ; the final prediction is given by matrix multiplying the data \n",
    "    #with the two set of weights, making the intermediate values non-negative (RELU activation function)\n",
    "\n",
    "    # calculate the mean squared error (MSE)\n",
    "    error =  # your code here\n",
    "\n",
    "    \n",
    "    writer.add_scalar(tag=\"Last run\",scalar_value= error, global_step = iteration)\n",
    "    writer.add_histogram(\"error distribution\",error)\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "   \n",
    "    #error.WHAT_FUNCTION_here ?\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # use w1.grad to update w2 according to the gradient descent formula\n",
    "        # use w2.grad to update w2 according to the gradient descent formula\n",
    "        # also use the learning_rate you set before!\n",
    "        w1 -= # your code here\n",
    "        w2 -= # your code here\n",
    "        \n",
    "    if iteration % 50 == 0:\n",
    "        print(\"Iteration: %d - Error: %.4f\" % (iteration, error))\n",
    "        w1_array.append(w1.cpu().detach().numpy())\n",
    "        w2_array.append(w2.cpu().detach().numpy())\n",
    "        errors.append(error.cpu().detach().numpy())\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    if loss_value < 1e-6:\n",
    "        print(\"Stopping gradient descent, algorithm converged, MSE loss is smaller than 1E-6\")\n",
    "        break\n",
    "        \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
